# Image & Audio Conversion Using Pseudo Hilbert Curves<br/>
Around the world, there are 285 million people who suffer from some form of visual impairment. Of this, 39 million are completely blind while 246 million have low-vision. Due to brain plasticity, it is common for the brain to re-organize itself to enhance other senses, especially auditory. With this heightened capability, an algorithm could create an association between color and sound. If someone were to learn this association, then it would be possible to construct a mental image through entirely sound. This could help people who are differently abled perceive the world around them.

This process works though a computer that is able to take an image, translate it to pixel values on the display, and then use the algorithm to utilize those pixel values to produce sounds. The way this works is through a program that is able to produce a unique audio clip for every possible pixel value. Computers display color based on three characteristics: red, green, and blue (otherwise known as RGB value). However, the human eye uses hue, saturation, and brightness (otherwise known as HSB value) to perceive images. By converting RGB pixel values into HSB, the association is much more easier to learn because the brain is receiving direct input that describes these values as supposed to unscrambling and deciphering RGB color. Not only this, but with this HSB method, similar looking colors would “sound” similar too, making this association much more effective.

HSB color has three dimensions, while sound only has two: frequency and amplitude. To tackle this problem, separate audio channels to describe the values could be utilized. Hue could play a sound function in the left ear, saturation one in the right, and brightness could control the volume via the amplitude. Research has shown that humans process any sound signal separately in each ear. This means that two audio channels would not cross-interfere with each other.

The pattern of how the computer translates pixel color into sound across an image is important too. The rudimentary solution would be to develop a reader that goes row by row, column or column, or even a “snake curve” that crawls its way up an image. However, there are a couple of limitations with these techniques. For one, the user would not have any sense of when the computer shifted to a new row or column unless there is another auditory cue involved. Another problem is that an increased resolution of the same image would “sound” completely different. Thus, the same image taken on two different cameras would require re-processing the sound. To solve this problem, a geometry known as a Hilbert curve (a plane-filling fractal line) can be used. Pseudo-Hilbert curves (PHC) don’t fill in an infinite space, but instead fill in squares that fit a 2<sup>_n_</sup> by 2<sup>_n_</sup> area. If you choose any point on this _n_-dimension square, then it will approach an exact point as n increases (Hilbert Curve = lim<sub>_n→∞_</sub> PHC<sub>_n_</sub>). The way this pattern works makes images of different resolutions sound similar. PHCs build upon each other continuously, which means the algorithm doesn’t need any additional audio cues to indicate a change in direction while the user creates a mental image.

This algorithm can be integrated with modern day smart-wear technology to help people see when needed. It certainly has the potential to help millions of people around the world.

Find full notebook and code breakdown at https://community.wolfram.com/groups/-/m/t/1862464.
